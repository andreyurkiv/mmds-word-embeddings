{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, Word2Vec\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy data from cloud storage to cluster hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop distcp gs:///ukwiki_*.csv hdfs://"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"WikiParse\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download stop words and service words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_df = spark.read.csv(\"gs:///stop_words.csv\").toPandas()\n",
    "sr_df = spark.read.csv(\"gs:///service_words.csv\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = sw_df['_c0'].tolist()\n",
    "service_words = sr_df['_c0'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read all of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batch(offset, limit):\n",
    "    fractions = []\n",
    "    for i in range(offset, limit):\n",
    "        print(\"Downloading fraction number {}...\".format(i))\n",
    "        df = spark.read.format(\"csv\") \\\n",
    "                        .option(\"header\", \"true\") \\\n",
    "                        .option(\"encoding\", \"UTF-8\") \\\n",
    "                        .load(\"hdfs:///ukwiki_\" + str(i) + \".csv\")\n",
    "        fractions.append(df)\n",
    "    return fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = reduce(lambda x, y: x.union(y), read_batch(0, 91))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Create training pipeline for word2vec model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline():\n",
    "    def __init__(self, df, stop_words, service_words):\n",
    "        self.stop_words = stop_words\n",
    "        self.service_words = service_words\n",
    "        self.df = df\n",
    "        self.vector_df = None\n",
    "    \n",
    "    def fit(self, sample=1):\n",
    "        word2Vec = Word2Vec(vectorSize=100, seed=42, inputCol='text', outputCol='model')\n",
    "        \n",
    "        if sample == 1:\n",
    "            return word2Vec.fit(self.vector_df)\n",
    "        \n",
    "        print(\"Start sempling\")\n",
    "        data = self.vector_df.sample(False, sample, seed=0)\n",
    "        print(\"End sempling\")\n",
    "        \n",
    "        return word2Vec.fit(data)\n",
    "    \n",
    "    def preprocess(self):\n",
    "        # clean data\n",
    "        df_trip = self.df.select(['Title', 'Text'])\\\n",
    "            .withColumn('Text', regexp_replace('Text', '[§»«·&\\~.a-zA-Z^=\\-\\\"<>!?:;{}()\\[\\]/|%0-9\\\\\\+\\*#_]+', ' '))\\\n",
    "            .withColumn('Text', regexp_replace('Text', '\\'{3}', ' '))\\\n",
    "            .withColumn('Text', regexp_replace('Text', '[—−]', ' '))\\\n",
    "            .withColumn('Text', regexp_replace('Text', '[^а-яА-ЯіІіІєЄҐґїЇ\\s]', ''))\\\n",
    "            .withColumn('Text', regexp_replace('Text', '\\s+', ' '))\\\n",
    "            .select([trim(lower(col('Title'))).alias('Title'), trim(lower(col('Text'))).alias('Text')])\n",
    "        \n",
    "        # tokenize data\n",
    "        tokenizer = Tokenizer(inputCol=\"Text\", outputCol=\"Vector\")\n",
    "        self.vector_df = tokenizer.transform(df_trip).select(\"vector\")\n",
    "        self.vector_df.show(5)\n",
    "        \n",
    "        # remove stop words\n",
    "        self.vector_df = self.__remove_stop_words(\"vector\", \"vector_no_stopwords\", self.stop_words)\n",
    "        \n",
    "        # remove service words\n",
    "        self.vector_df = self.__remove_stop_words(\"vector_no_stopwords\", \"text\", self.service_words)\n",
    "        \n",
    "    def __remove_stop_words(self, in_col, out_col, words_list):\n",
    "        remover = StopWordsRemover(inputCol=in_col, outputCol=out_col, stopWords=words_list)\n",
    "        stopwords = remover.getStopWords() \n",
    "        \n",
    "        vector_no_stopw_df = remover.transform(self.vector_df).select(out_col)\n",
    "        vector_no_stopw_df.show(5)\n",
    "        \n",
    "        return vector_no_stopw_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_batch(batch, stop_words, service_words):\n",
    "        return Pipeline(batch, stop_words, service_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Pipeline.from_batch(fractions, stop_words, service_words)\n",
    "p.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training word2vec model over fraction of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 101, 5):\n",
    "    fraction = i/100\n",
    "    print(\"Training {} fraction\".format(fraction))\n",
    "    model = p.fit(sample=i/100)\n",
    "    print(\"Saving word2vec of {} fraction\".format(fraction))\n",
    "    model.write().save(\"hdfs:///w2v/word2vec_{}\".format(i))\n",
    "    print(\"Copying word2vec model to gcloud storage\")\n",
    "    !eval {\"hadoop distcp hdfs:///w2v/word2vec_{i} gs:///w2v/word2vec_{i}\".format(i=i)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}